'''
The default implementation of jax.scipy.sparse.linalg.gmres appears to be flawed and gives memory bugs for no reason.
Here we implement our own GMRES routines.
'''

import jax
import jax.flatten_util
import jax.numpy as jnp

from scipy.io import savemat #For debugging

def gmres(A, b, inner, outer=1, preconditioner_list=[] ):
    '''
    Generalized Minimal RESidual (GMRES) method for solving Ax=b

    A matrix-free iterative method for approximately solving the linear 
    system Ax=b. Construct an orthonormal basis with power iteration of A
    and minimize the L_2 error for a solution constrained to this subspace.
    The goal of this implementation is to be JIT-able
    
    Parameters
    ----------
    A : callable
        Function that returns a matrix-vector product
    b : ndarray
        The right hand side vector of Ax=b. Assumed to be flat.
    inner : int
        Dimension of the Krylov subspace / number of times A will be evaluated
    outer : int
        Number of times to restart Krylov subspace generation.
        Restarting is a generally terrible idea in my experience.
        Only do multiple outer iterations if you are memory limited and desparate.
    preconditioner_list : List of callables 
        an arbitrary length list of preconditioners (M1, M2, ...) to apply to the linear system.
        GMRES is then applied to the system  Mn*...*M2*M1*A*x = Mn*...*M2*M1*b
        Each M is a function handle so it evaluates via M(v)
    '''
        
    n = b.size #number of elements in this square system
    Q = jnp.zeros((n, inner+1))
    H = jnp.zeros((inner+1, inner))

    #Define JIT-able preconditioning
    #precond_i = lambda i, v : preconditioner_list[i](v)
    if len(preconditioner_list) > 0:
        #precond = lambda v : jax.lax.fori_loop( 0, len(preconditioner_list), precond_i, v)
        def precond(v):
            def body(i, v):
                return jax.lax.switch(i, preconditioner_list, v)
            return jax.lax.fori_loop(0, len(preconditioner_list), body, v)
    else:
        precond = lambda v : v

    #Apply preconditioners to right hand side
    b = precond(b)
    
    #Use the right hand side to generate the Krylov subspace
    Q = Q.at[:,0].set( b / jnp.linalg.norm(b) )

    def orthogonalize(u,v):
        dot = jnp.dot(u,v)
        u = u - dot*v
        return dot, u

    def arnoldi_iteration(k, state):
        Q, H = state
        Aq = A(Q[:,k])
        Aq = precond(Aq)
    
        def update(j, state):
            Aq, H = state
            hj, Aq = orthogonalize( Aq, Q[:,j])
            H = H.at[j, k].set(hj)
            return Aq, H
        
        #Orthogonalize with respect to all previous vectors
        Aq, H = jax.lax.fori_loop(0,k+1,update, (Aq,H))

        #Optionally reorthogonalize for numerical stability
        #TODO

        #Use the rest of Aq to define the next basis vector
        hk1 = jnp.linalg.norm(Aq)
        H = H.at[k+1, k].set(hk1)
        Q = Q.at[:,k+1].set(Aq / hk1)
        return Q, H

    #Generate a Krylov subspace
    Q, H = jax.lax.fori_loop(0, inner, arnoldi_iteration, (Q,H)) 
    
    #Project b onto the orthonormal basis
    b2 = Q.transpose() @ b
    y, _, _, _ = jnp.linalg.lstsq(H, b2, rcond=None)
    x = Q[:, :inner] @ y

    #Compute the relative residual the lazy (but accurate) way
    rel_res = jnp.linalg.norm( precond(A(x)) - b )/jnp.linalg.norm(b)
    return x, rel_res



def newton_gmres_hookstep( AtA, Atb, m, s, f, f0, J, b, input_dict ):
    '''
    PURPOSE:
    Compute a Newton-Raphson step by combining GMRES and hookstep. We exactly minimize the loss function

    L(x,t) = 1/2 sum( (Ax - b)^2 ) + 1/2 t (sum(x^2) - s)
    
    where Q is an orthonormal basis generated by Arnoldi iteration of A^TA, 
    t is a Lagrange multiplier, and s is the desired size of the Newton-Raphson step. 

    INPUT:
    AtA - a linear operator producing the action AT(A())
    Atb - right hand side vector
    m - dimension of Krylov subspace
    s - initial guess at 2-norm of Newton step
    f - function handle to objective function
    f0- current value of objective function
    J - function handle for Jacobian
    input_dict - the state dictionary.

    DETAILS:
    This is an all-in-one function for finding a good Newton-Raphson step.
    '''
    
    n = Atb.size #number of elements in this square system
    Q = jnp.zeros((n,m))
    H = jnp.zeros((m,m))

    Q = Q.at[:,0].set( Atb / jnp.linalg.norm(Atb) )
        
    def orthogonalize(u,v):
        #Prevent code redundancy.
        w = jnp.dot(u,v)
        u = u - w*v
        return w, u

    for k in range(m):
        Aq = AtA(Q[:,k])
    
        for j in range(k+1):
            hj, Aq = orthogonalize( Aq, Q[:,j])
            H = H.at[j, k].set(hj)
            
        #Reorthogonalize a couple of times for improved numerical stability
        for _ in range(2):
            for j in range(k+1):
                _, Aq = orthogonalize( Aq, Q[:,j])

        if k != m-1:
            hk1 = jnp.linalg.norm(Aq)
            H = H.at[k+1, k].set(hk1)
            Q = Q.at[:,k+1].set(Aq / hk1)
    
    #Project Atb into the Krylov subspace
    b2 = Q.transpose() @ Atb

    #Compute the eigenvectors and eigenvalues of H
    D, V = jnp.linalg.eigh(H)

    #Project b into the eigenvectors
    b2 = V.transpose() @ b2

    #Use bisection to find t such that g(t) = 0
    def bisection(g, lower, upper, maxit):
        #For the problem of interest, if g(0) is positive we can just return 0
        if g(0) > 0:
            print("No bisection needed")
            t = 0
        else:
            for _ in range(maxit):
                t = (lower + upper)/2
                if g(t) > 0:
                    upper = t
                else:
                    lower = t
        return t
        
    #Unpack the state
    x0, unravel_fn = jax.flatten_util.ravel_pytree(input_dict)

    #Assume the f passed maps dictionaries to dictionaries
    f_vector = lambda x: jax.flatten_util.ravel_pytree(f( unravel_fn(x) ))[0]

    while True:
        #Define a function (dependent on s which changes each iteration of this loop)
        g = lambda t : s - jnp.linalg.norm(b2/(D+t))
        
        #Do bisection to find the root if needed
        t = 0 if g(0) > 0 else bisection(g, lower=0, upper=jnp.linalg.norm(b)/s, maxit=128)
        
        #Solve for the step in physical coordinates
        step = Q @ (V @ (b2/(D+t)))

        #Check if the step decreases the norm of the objective function
        f2 = f_vector(x0 - step)
 
        if jnp.linalg.norm(f2) < jnp.linalg.norm(f0):
            s = s * 2 #Try increasing the acceptable size of step
            break
        else:
            #This is not a good step. Try a smaller Newton step
            s = s / 2
            
    #Compute both relative residuals the lazy (but accurate) way
    rel_res_1 = jnp.linalg.norm( AtA(step) - Atb )/jnp.linalg.norm(Atb)
    Jv = J(step)
    rel_res_2 = jnp.linalg.norm( Jv + b )/jnp.linalg.norm(b) #sign change +b for reasons

    #Update the state
    input_dict = unravel_fn( x0 - step )

    return input_dict, s, rel_res_1, rel_res_2



def newton_gmres_hookstep_v2( A, b, m, s, objective, input_dict ):
    '''
    Iteratively solve for a Newton-Raphson step with a bound on the norm of the solution.

    Generate a Krylov subspace in the usual GMRES way, but solve the linear system with
    a bound on the 2-norm of the step. 

    INPUT:
    A : callable
        function for evaluating the matrix-vector product
    b : array
        right hand side Ax=b
        This is the flattened output of objective(input_dict)
    m : int
        dimension of Krylov subspace
    s : float
        upper bound on the 2-norm of the newton step
    objective: callable
        Acts on a dictionary of inputs and returns a dictionary of tensors that we want to be zero.
    input_dict : dict
        state dictionary.

    Returns:
    
    '''
    
    n = b.size #number of elements in this square system
    Q = jnp.zeros((n,m+1))
    H = jnp.zeros((m+1,m))

    Q = Q.at[:,0].set( b / jnp.linalg.norm(b) )
        
    def orthogonalize(u,v):
        dot = jnp.dot(u,v)
        u = u - dot*v
        return dot, u

    for k in range(m):
        Aq = A(Q[:,k])
        for j in range(k+1):
            hj, Aq = orthogonalize( Aq, Q[:,j])
            H = H.at[j,k].set(hj)
        #Reorthogonalize a couple of times for improved numerical stability
        for _ in range(2):
            for j in range(k+1):
                _, Aq = orthogonalize( Aq, Q[:,j])

        hk1 = jnp.linalg.norm(Aq)
        H = H.at[k+1, k].set(hk1)
        Q = Q.at[:,k+1].set(Aq / hk1)
    
    #Project b into the Krylov subspace
    b2 = Q.transpose() @ b

    #Compute the economy SVD of H
    U, S, Vh = jnp.linalg.svd(H,full_matrices=False)
    
    #Solve the least squares problem by dividing by the singular values
    y = U.transpose() @ b2
    y = y/S #For small singular values, this will be dangerous! (Hence the rest of this function)
    y = Vh.transpose() @ y

    #Do a while loop to find a Newton step that actually decreases the norm of 
    #the objective function. At each iteration of this loop, s will change.
    while(True):
        if jnp.linalg.norm(y) > s*s:
            #We have a step that is too big! We can find the correct regularization scheme
            b3 = H.transpose() @ b2
            b4 = Vh @ b3
            
            g = lambda t : s - jnp.linalg.norm( b4/(S*S+t) )

            def bisection(g, lower, upper, maxit):
                #For the problem of interest, if g(0) is positive we can just return 0
                if g(0) > 0:
                    print("No bisection needed")
                    t = 0
                else:
                    for _ in range(maxit):
                        t = (lower + upper)/2
                        if g(t) > 0:
                            upper = t
                        else:
                            lower = t
                return t
            
            lower_bound = 0
            upper_bound = jnp.sqrt(m) * jnp.linalg.norm(b4)/s
            t = bisection(g, lower_bound, upper_bound, maxit=128)

            z = b4/(S*S + t)
            #print(f"DEBUG: Relative error in 2-norm of z is {jnp.linalg.norm(z)/s - 1}")
            y = Vh.transpose() @ z
        #This quantity is x, the solution to Ax=b, but I want to reserve x for the state vector.
        #Call it dx for change in x, since this is a Newton step.
        dx = Q[:,:-1] @ y
        
        #Unpack the state
        x, unravel_fn = jax.flatten_util.ravel_pytree(input_dict)

        #Assume the f passed maps dictionaries to dictionaries
        f = lambda x: jax.flatten_util.ravel_pytree(objective( unravel_fn(x) ))[0]

        f_new = f(x - dx)
        
        if jnp.linalg.norm(f_new) < jnp.linalg.norm(b):
            #We decreased the objective function!
            #Try increasing the acceptable size of step new time
            s = s * 2
            break
        else:
            #This is not a good step. Try a smaller Newton step
            s = s / 2
            
    #Compute both relative residuals the lazy (but accurate) way
    rel_res = jnp.linalg.norm( A(dx) - b )/jnp.linalg.norm(b)
    
    #Update the state
    input_dict = unravel_fn( x - dx )

    return input_dict, s, rel_res



def block_gmres(A, b, m, B, tol=1e-8, iteration=0):
    '''
    PURPOSE:
    Typical GMRES solves the linear system Ax=b by constructing a Krylov subspace
    K = {v, Av, A^2v, A^3v, ...}. 
    This is terrible, because you need to wait for Av to finish evaluating before starting A^2v.
    This subspace generation is too sequential and does not abuse the massively parallel computers 
    we have in the modern world.

    block_gmres will instead iterate an initial block of vectors 
    K = {B, AB, A^2B, ...} where B is the block. K is taken to be the span of the columns of this set.
    B is unrelated to b, although you might want B to contain b as a column.
        
    INPUT:
    A - a linear operator
    b - right hand side vector
    m - number of times to multiply our block by A
    B - block of vectors
    '''

    #block size
    n = B.shape[0]
    s = B.shape[1]

    Q = jnp.zeros((n, s*(m+1)) )
    H = jnp.zeros((s*(m+1), s*m))

    #Orthonormalize the block before we begin function evaluation
    B, _ = jnp.linalg.qr(B, mode='reduced')

    #Use the orthonormal block to start off our orthonormal basis
    Q = Q.at[:, 0:s].set(B)

    #Apply the operator m times
    for k in range(m):
        #Apply the linear operator to the block
        C = A(B)

        #Loop over the columns of C
        for i in range(s):
            #i is the index of the column relative to C
            #The corresponding index of Q is needed. Call this a.
            a = k*s + i
            for j in range(a+s):
                #Project current vector
                h = jnp.dot(Q[:,j], C[:,i])
                
                #Update the Hessenberg matrix
                H = H.at[j, a].set(h)
                C = C.at[:, i].set(C[:,i] - h * Q[:,j] )

            #Check if we have a new vector to add
            h = jnp.linalg.norm(C[:,i])
            if h < tol:
                print("Oh no")
                break

            H = H.at[a+s,a].set(h)
            Q = Q.at[:,a+s].set(C[:,i]/h)
            C = C.at[:,i].set(C[:,i]/h)
        
            #Reorthogonalize
            #I found this MANDATORY for numerical stability for subspaces with more than ~50 vectors
            for j in range(a+s):
                h = jnp.dot(Q[:,j], Q[:,a+s])
                Q = Q.at[:,a+s].set(Q[:,a+s] - h * Q[:,j])
        B = C
    #For debugging
    #savemat(f"debug/bgmres_{iteration}.mat", {"H": H, "Q": Q} )

    #Project b onto the orthonormal basis
    b2 = Q.T @ b
    
    U, s, Vh = jnp.linalg.svd(H, full_matrices=False)

    b2 = U.T @ b2
    inv_s = 1 / s

    #NEVER INCREASE THE SIZE OF A STABLE DIRECTION
    s_min = 1.0 #smallest singular value we are comfortable inverting
    inv_s = inv_s.at[ s < s_min ].set(1)

    b2 = b2 * inv_s
    y  = Vh.T @ b2

    x = Q[:, :y.shape[0]] @ y
    return x



def adjoint_GMRES( A, A_t, b, m, n, inner, outer=1, precond_left=[], x0_fn=lambda x, _: x, seed=0):
    """
    Iteratively solve the linear system Ax=b

    Parameters
    ----------
    A : callable
        Evaluates the matrix-vector product (MVP) via A(x)
    A_t: callable
        Evaluates the vector-matrix product (VMP) via A_t(x)
    b: ndarray
        A 1-dimensional vector that is the right hand side of Ax=b
    m: int
        The number of rows of A
    n: int
        The number of columns of A
    inner: int
        The size of the Krylov subspace we use to approximate x
    outer: int
        How many times should we restart Krylov subspace construction?
    precond_left: list of callable
        List of matrix-free preconditioning operators. Each callable should accept:
        - x (ndarray): The input vector
        - mode (str): One of "no_trans" or "trans" indicating whether to apply the operator or its transpose.
        Returns an ndarray representing the result of the MVP or VMP.
    x0_fn: callable
        Determines the initial vector of the Krylov subspace via x0_fn(b,key), where b is the current right hand side
        at a given outer iteration and key = jax.random.PRNGKey(seed) is used for reproducible random number generation.
        The default x0_fn wil just return b, which is a common choice for GMRES.
    seed: int
        Used to create a key = jax.random.PRNGKey(seed) for initializing the Krylov subspace if a random x0_fn is provided by the user.

    Returns
    -------
    x: ndarray
        The 1D array approximating the solution to Ax=b
    """
    
    #The three matrices we construct every inner iteration
    B = jnp.zeros((inner+1, inner))
    U = jnp.zeros((m, inner+1))
    V = jnp.zeros((n, inner))

    #The approximate solution to Ax=b
    #We will build the solution iteratively.
    x = jnp.zeros((n,))

    #Store b before we modify it so we can monitor the relative residual of Ax=b
    b0 = jnp.copy(b)
 
    #Apply left preconditioners to b
    for M in precond_left:
        b = M(b, "no_trans")

    #Create the pseudorandom number key
    key = jax.random.PRNGKey(seed)

    for outer_iteration in range(outer):
        #Split the key so we get a new random vector each iteration
        key, subkey = jax.random.split(key)

        #Evaluate the x0_fn to get the initial vector of our Krylov subsapce
        x0 = x0_fn(b, subkey)

        #Generate our orthonormal basis vectors with x0
        U = U.at[:,0].set( x0 / jnp.linalg.norm(x0) )

        #Start power iteration
        for i in range(inner):

            #Apply (Mn... M2 M1 A)^T = A^T M1^T M2^T ... Mn^T
            Au = U[:,i]
            for M in reversed(precond_left):
                Au = M(Au, "trans")
            Au = A_t(Au)

            #Orthogonalize with respect to previous V
            for j in range(i):
                B = B.at[i,j].set(jnp.dot( V[:,j], Au ))
                Au = Au - B[i,j]*V[:,j]
            
            #Reorthogonalize to ensure numerical stability
            #These dot products are not the computational bottleneck
            for _ in range(2):
                for j in range(i):
                    Bij = jnp.dot( V[:,j], Au )
                    Au = Au - Bij*V[:,j]
                
            #Define new vector of V
            B = B.at[i,i].set(jnp.linalg.norm(Au))
            V = V.at[:,i].set( Au / B[i,i] )

            #Apply MA
            Av = A(V[:,i])
            for M in precond_left:
                Av = M(Av, "no_trans")

            #Orthogonalize    
            for j in range(i+1):
                B  = B.at[j,i].set(jnp.dot( U[:,j], Av ))
                Av = Av - B[j,i]*U[:,j]
        
            #Reorthogonalize to ensure numerical stability
            for _ in range(2):
                for j in range(i+1):
                    Bji  = jnp.dot( U[:,j], Av )
                    Av = Av - Bji*U[:,j]

            B = B.at[i+1,i].set(jnp.linalg.norm(Av))
            U = U.at[:,i+1].set(Av / B[i+1,i])

        #Our bidiagonalization is complete (for this iteration)
        #Project the current right hand side b into U
        b2 = U.transpose() @ b
        
        #Determine how much of b lives in U for informed Krylov choices
        projection_b = jnp.linalg.norm(b2) / jnp.linalg.norm(b)
  
        #Solve least squares problem
        #TODO, just do Given's rotations and JIT it
        y, _, _, _ = jnp.linalg.lstsq(B, b2, rcond=None)
        
        #Lift y into the full space (multiply by V) 
        dx = V @ y

        #Add this to our guess of x
        x = x + dx

        #Update b. Do a matrix evaluation. Eh, it's cheap enough.
        Ax = A(x)
        for M in precond_left:
                Ax = M(Ax, "no_trans")
        b = b0 - Ax

        rel_res = jnp.linalg.norm(b) / jnp.linalg.norm(b0)
        print(f"outer iter {outer_iteration:03d}: relative residual = {rel_res:.6e}, |b2|/|b|={projection_b:.6e}")
    
        savemat("debug_adjoint_GMRES.mat", {"U": U, "V": V, "B": B, "b": b})

    return x





