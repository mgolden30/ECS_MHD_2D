'''
The default implementation of jax.scipy.sparse.linalg.gmres is flawed and gives memory bugs for no reason.
Here we implement our own GMRES routines.
'''

import jax
import jax.flatten_util
import jax.numpy as jnp

from scipy.io import savemat



def gmres(A, b, m, s_min, tol=1e-8, preconditioner_list=[], output_index=0 ):
    '''
    PURPOSE:
    Solve the linear system Ax=b by constructing a Krylov subspace.
    
    INPUT:
    A - a linear operator
    b - right hand side vector
    m - dimension of Krylov subspace
    Q0 - initial vector of Q

    preconditioner_list - an arbitrary length list of preconditioners (M1, M2, ...) to apply to the linear system.
    GMRES is then applied to the system  Mn*...*M2*M1*A*x = Mn*...*M2*M1*b
    Each M is a function handle so it evaluates via M(v)
    '''
        
    n = b.size #number of elements in this square system
    Q = jnp.zeros((n, m+1))
    H = jnp.zeros((m+1, m))

    #Apply preconditioners to both b and Q0
    for M in preconditioner_list:
        b  = M(b)
        Q0 = M(Q0)

    Q = Q.at[:,0].set( b / jnp.linalg.norm(b) )
    
    #A unit vector e1
    e1 = jnp.zeros([m+1,]).at[0].set(1)

    def orthogonalize(u,v):
        #Prevent code redundancy.
        w = jnp.dot(u,v)
        u = u - w*v
        return w, u

    for k in range(m):
        Aq = A(Q[:,k])
        for M in preconditioner_list:
            Aq = M(Aq)

        for j in range(k+1):
            hj, Aq = orthogonalize( Aq, Q[:,j])
            H = H.at[j, k].set(hj)
            
        #Reorthogonalize a couple of times
        for _ in range(2):
            for j in range(k+1):
                _, Aq = orthogonalize( Aq, Q[:,j])
            
        hk1 = jnp.linalg.norm(Aq)
        H = H.at[k+1, k].set(hk1)
        Q = Q.at[:,k+1].set(Aq / hk1)

    #Project b onto the orthonormal basis
    b2 = Q.transpose() @ b

    if s_min == 0:
        y, _, _, _ = jnp.linalg.lstsq(H, b2, rcond=None)
    else:
        #There is some singular value threshold we should respect
        U, s, Vh = jnp.linalg.svd(H, full_matrices=False)
        b2 = U.T @ b2
        inv_s = 1 / s
        inv_s = inv_s.at[ s < s_min ].set(1) #NEVER INCREASE THE SIZE OF A STABLE DIRECTION
        b2 = b2 * inv_s
        y  = Vh.T @ b2
        
    filename = f"gmres_debug_{output_index}.mat"
    savemat(filename, {"H": H, "b": b2, "f": b, "Q": Q})

    x = Q[:, :m] @ y

    #Compute the relative residual the lazy (but accurate) way
    rel_res = jnp.linalg.norm( A(x) - b )/jnp.linalg.norm(b)
    return x, rel_res



def newton_gmres_hookstep( AtA, Atb, m, s, f, f0, J, b, input_dict ):
    '''
    PURPOSE:
    Compute a Newton-Raphson step by combining GMRES and hookstep. We exactly minimize the loss function

    L(x,t) = 1/2 sum( (Ax - b)^2 ) + 1/2 t (sum(x^2) - s)
    
    where Q is an orthonormal basis generated by Arnoldi iteration of A^TA, 
    t is a Lagrange multiplier, and s is the desired size of the Newton-Raphson step. 

    INPUT:
    AtA - a linear operator producing the action AT(A())
    Atb - right hand side vector
    m - dimension of Krylov subspace
    s - initial guess at 2-norm of Newton step
    f - function handle to objective function
    f0- current value of objective function
    J - function handle for Jacobian
    input_dict - the state dictionary.

    DETAILS:
    This is an all-in-one function for finding a good Newton-Raphson step.
    '''
    
    n = Atb.size #number of elements in this square system
    Q = jnp.zeros((n,m))
    H = jnp.zeros((m,m))

    Q = Q.at[:,0].set( Atb / jnp.linalg.norm(Atb) )
        
    def orthogonalize(u,v):
        #Prevent code redundancy.
        w = jnp.dot(u,v)
        u = u - w*v
        return w, u

    for k in range(m):
        Aq = AtA(Q[:,k])
    
        for j in range(k+1):
            hj, Aq = orthogonalize( Aq, Q[:,j])
            H = H.at[j, k].set(hj)
            
        #Reorthogonalize a couple of times for improved numerical stability
        for _ in range(2):
            for j in range(k+1):
                _, Aq = orthogonalize( Aq, Q[:,j])

        if k != m-1:
            hk1 = jnp.linalg.norm(Aq)
            H = H.at[k+1, k].set(hk1)
            Q = Q.at[:,k+1].set(Aq / hk1)
    
    #Project Atb into the Krylov subspace
    b2 = Q.transpose() @ Atb

    #Compute the eigenvectors and eigenvalues of H
    D, V = jnp.linalg.eigh(H)

    #Project b into the eigenvectors
    b2 = V.transpose() @ b2

    #Use bisection to find t such that g(t) = 0
    def bisection(g, lower, upper, maxit):
        #For the problem of interest, if g(0) is positive we can just return 0
        if g(0) > 0:
            print("No bisection needed")
            t = 0
        else:
            for _ in range(maxit):
                t = (lower + upper)/2
                if g(t) > 0:
                    upper = t
                else:
                    lower = t
        return t
        
    #Unpack the state
    x0, unravel_fn = jax.flatten_util.ravel_pytree(input_dict)

    #Assume the f passed maps dictionaries to dictionaries
    f_vector = lambda x: jax.flatten_util.ravel_pytree(f( unravel_fn(x) ))[0]

    while True:
        #Define a function (dependent on s which changes each iteration of this loop)
        g = lambda t : s - jnp.linalg.norm(b2/(D+t))
        
        #Do bisection to find the root if needed
        t = 0 if g(0) > 0 else bisection(g, lower=0, upper=jnp.linalg.norm(b)/s, maxit=128)
        
        #Solve for the step in physical coordinates
        step = Q @ (V @ (b2/(D+t)))

        #Approximate the Jacobian vector product with this step
        Jv_fd =  f_vector( x0 - step ) - f0

        #Compute the exact Jacobian vector product
        Jv = J(-step)

        #Compute a dimensionless "distance" between these vectors.  
        distance = jnp.linalg.norm( Jv_fd - Jv ) / jnp.sqrt( jnp.linalg.norm(Jv) * jnp.linalg.norm(Jv_fd) )

        print(f"distance = {distance}")
        if distance > 0.1:
            #This is not a good linear approximation. Try a smaller Newton step
            s = s / 2
        else:
            #We have an acceptable step! Try taking it.
            s = s * 1.25 #Try increasing the acceptable size of step for next time
            break

    #Compute both relative residuals the lazy (but accurate) way
    rel_res_1 = jnp.linalg.norm( AtA(step) - Atb )/jnp.linalg.norm(Atb)
    rel_res_2 = jnp.linalg.norm( Jv + b )/jnp.linalg.norm(b) #sign change +b for reasons

    #Update the state
    input_dict = unravel_fn( x0 - step )

    return input_dict, s, rel_res_1, rel_res_2


def block_gmres(A, b, m, B, tol=1e-8, iteration=0):
    '''
    PURPOSE:
    Typical GMRES solves the linear system Ax=b by constructing a Krylov subspace
    K = {v, Av, A^2v, A^3v, ...}. 
    This is terrible, because you need to wait for Av to finish evaluating before starting A^2v.
    This subspace generation is too sequential and does not abuse the massively parallel computers 
    we have in the modern world.

    block_gmres will instead iterate an initial block of vectors 
    K = {B, AB, A^2B, ...} where B is the block. K is taken to be the span of the columns of this set.
    B is unrelated to b, although you might want B to contain b as a column.
        
    INPUT:
    A - a linear operator
    b - right hand side vector
    m - number of times to multiply our block by A
    B - block of vectors
    '''

    #block size
    n = B.shape[0]
    s = B.shape[1]

    Q = jnp.zeros((n, s*(m+1)) )
    H = jnp.zeros((s*(m+1), s*m))

    #Orthonormalize the block before we begin function evaluation
    B, _ = jnp.linalg.qr(B, mode='reduced')

    #Use the orthonormal block to start off our orthonormal basis
    Q = Q.at[:, 0:s].set(B)

    #Apply the operator m times
    for k in range(m):
        #Apply the linear operator to the block
        C = A(B)

        #Loop over the columns of C
        for i in range(s):
            #i is the index of the column relative to C
            #The corresponding index of Q is needed. Call this a.
            a = k*s + i
            for j in range(a+s):
                #Project current vector
                h = jnp.dot(Q[:,j], C[:,i])
                
                #Update the Hessenberg matrix
                H = H.at[j, a].set(h)
                C = C.at[:, i].set(C[:,i] - h * Q[:,j] )

            #Check if we have a new vector to add
            h = jnp.linalg.norm(C[:,i])
            if h < tol:
                print("Oh no")
                break

            H = H.at[a+s,a].set(h)
            Q = Q.at[:,a+s].set(C[:,i]/h)
            C = C.at[:,i].set(C[:,i]/h)
        
            #Reorthogonalize
            #I found this MANDATORY for numerical stability for subspaces with more than ~50 vectors
            for j in range(a+s):
                h = jnp.dot(Q[:,j], Q[:,a+s])
                Q = Q.at[:,a+s].set(Q[:,a+s] - h * Q[:,j])
        B = C
    #For debugging
    #savemat(f"debug/bgmres_{iteration}.mat", {"H": H, "Q": Q} )

    #Project b onto the orthonormal basis
    b2 = Q.T @ b
    
    U, s, Vh = jnp.linalg.svd(H, full_matrices=False)

    b2 = U.T @ b2
    inv_s = 1 / s

    #NEVER INCREASE THE SIZE OF A STABLE DIRECTION
    s_min = 1.0 #smallest singular value we are comfortable inverting
    inv_s = inv_s.at[ s < s_min ].set(1)

    b2 = b2 * inv_s
    y  = Vh.T @ b2

    x = Q[:, :y.shape[0]] @ y
    return x



def adjoint_GMRES( A, A_t, b, m, n, inner, outer=1, precond_left=[], x0_fn=lambda x, _: x, seed=0):
    """
    Iteratively solve the linear system Ax=b

    Parameters
    ----------
    A : callable
        Evaluates the matrix-vector product (MVP) via A(x)
    A_t: callable
        Evaluates the vector-matrix product (VMP) via A_t(x)
    b: ndarray
        A 1-dimensional vector that is the right hand side of Ax=b
    m: int
        The number of rows of A
    n: int
        The number of columns of A
    inner: int
        The size of the Krylov subspace we use to approximate x
    outer: int
        How many times should we restart Krylov subspace construction?
    precond_left: list of callable
        List of matrix-free preconditioning operators. Each callable should accept:
        - x (ndarray): The input vector
        - mode (str): One of "no_trans" or "trans" indicating whether to apply the operator or its transpose.
        Returns an ndarray representing the result of the MVP or VMP.
    x0_fn: callable
        Determines the initial vector of the Krylov subspace via x0_fn(b,key), where b is the current right hand side
        at a given outer iteration and key = jax.random.PRNGKey(seed) is used for reproducible random number generation.
        The default x0_fn wil just return b, which is a common choice for GMRES.
    seed: int
        Used to create a key = jax.random.PRNGKey(seed) for initializing the Krylov subspace if a random x0_fn is provided by the user.

    Returns
    -------
    x: ndarray
        The 1D array approximating the solution to Ax=b
    """
    
    #The three matrices we construct every inner iteration
    B = jnp.zeros((inner+1, inner))
    U = jnp.zeros((m, inner+1))
    V = jnp.zeros((n, inner))

    #The approximate solution to Ax=b
    #We will build the solution iteratively.
    x = jnp.zeros((n,))

    #Store b before we modify it so we can monitor the relative residual of Ax=b
    b0 = jnp.copy(b)
 
    #Apply left preconditioners to b
    for M in precond_left:
        b = M(b, "no_trans")

    #Create the pseudorandom number key
    key = jax.random.PRNGKey(seed)

    for outer_iteration in range(outer):
        #Split the key so we get a new random vector each iteration
        key, subkey = jax.random.split(key)

        #Evaluate the x0_fn to get the initial vector of our Krylov subsapce
        x0 = x0_fn(b, subkey)

        #Generate our orthonormal basis vectors with x0
        U = U.at[:,0].set( x0 / jnp.linalg.norm(x0) )

        #Start power iteration
        for i in range(inner):

            #Apply (Mn... M2 M1 A)^T = A^T M1^T M2^T ... Mn^T
            Au = U[:,i]
            for M in reversed(precond_left):
                Au = M(Au, "trans")
            Au = A_t(Au)

            #Orthogonalize with respect to previous V
            for j in range(i):
                B = B.at[i,j].set(jnp.dot( V[:,j], Au ))
                Au = Au - B[i,j]*V[:,j]
            
            #Reorthogonalize to ensure numerical stability
            #These dot products are not the computational bottleneck
            for _ in range(2):
                for j in range(i):
                    Bij = jnp.dot( V[:,j], Au )
                    Au = Au - Bij*V[:,j]
                
            #Define new vector of V
            B = B.at[i,i].set(jnp.linalg.norm(Au))
            V = V.at[:,i].set( Au / B[i,i] )

            #Apply MA
            Av = A(V[:,i])
            for M in precond_left:
                Av = M(Av, "no_trans")

            #Orthogonalize    
            for j in range(i+1):
                B  = B.at[j,i].set(jnp.dot( U[:,j], Av ))
                Av = Av - B[j,i]*U[:,j]
        
            #Reorthogonalize to ensure numerical stability
            for _ in range(2):
                for j in range(i+1):
                    Bji  = jnp.dot( U[:,j], Av )
                    Av = Av - Bji*U[:,j]

            B = B.at[i+1,i].set(jnp.linalg.norm(Av))
            U = U.at[:,i+1].set(Av / B[i+1,i])

        #Our bidiagonalization is complete (for this iteration)
        #Project the current right hand side b into U
        b2 = U.transpose() @ b
        
        #Determine how much of b lives in U for informed Krylov choices
        projection_b = jnp.linalg.norm(b2) / jnp.linalg.norm(b)
  
        #Solve least squares problem
        #TODO, just do Given's rotations and JIT it
        y, _, _, _ = jnp.linalg.lstsq(B, b2, rcond=None)
        
        #Lift y into the full space (multiply by V) 
        dx = V @ y

        #Add this to our guess of x
        x = x + dx

        #Update b. Do a matrix evaluation. Eh, it's cheap enough.
        Ax = A(x)
        for M in precond_left:
                Ax = M(Ax, "no_trans")
        b = b0 - Ax

        rel_res = jnp.linalg.norm(b) / jnp.linalg.norm(b0)
        print(f"outer iter {outer_iteration:03d}: relative residual = {rel_res:.6e}, |b2|/|b|={projection_b:.6e}")
    
        savemat("debug_adjoint_GMRES.mat", {"U": U, "V": V, "B": B, "b": b})

    return x





